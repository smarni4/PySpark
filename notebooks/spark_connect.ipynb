{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9827c57-88ad-4e8a-a8bd-3f4289645448",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /Users/veera/Desktop/Git/PySpark/spark-3.5.1-bin-hadoop3/logs/spark-veera-org.apache.spark.sql.connect.service.SparkConnectServer-1-veeras-MBP.out\n",
      "failed to launch: nice -n 0 bash /Users/veera/Desktop/Git/PySpark/spark-3.5.1-bin-hadoop3/bin/spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.12:\n",
      "  \tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1541)\n",
      "  \tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "  \tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
      "full log in /Users/veera/Desktop/Git/PySpark/spark-3.5.1-bin-hadoop3/logs/spark-veera-org.apache.spark.sql.connect.service.SparkConnectServer-1-veeras-MBP.out\n"
     ]
    }
   ],
   "source": [
    "!./Desktop/Git/PySpark/spark-3.5.1-bin-hadoop3/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:$SPARK_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d779c19c-b801-4d49-83ea-954484310612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio-status\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "Collecting protobuf<6.0dev,>=5.26.1\n",
      "  Downloading protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.64.1\n",
      "  Downloading grpcio-1.64.1-cp39-cp39-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos>=1.5.5\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, grpcio, googleapis-common-protos, grpcio-status\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.59.3\n",
      "    Uninstalling grpcio-1.59.3:\n",
      "      Successfully uninstalled grpcio-1.59.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.2 which is incompatible.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 5.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed googleapis-common-protos-1.63.2 grpcio-1.64.1 grpcio-status-1.64.1 protobuf-5.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio-status\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession.builder.master(\"local[*]\").getOrCreate().stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2049914e-f22e-47c6-8348-0674a6ff91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veera/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/connect/session.py:185: UserWarning: segregated_call() takes at most 8 positional arguments (9 given)\n",
      "  warnings.warn(str(e))\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb6198-fa85-44fb-8662-ff3f4c36d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
