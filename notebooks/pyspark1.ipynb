{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3afd50-53ab-4114-a2ba-2b77b055d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom pyspark.sql import SparkSession\\nspark = SparkSession.builder.getOrCreate()\\nfrom datetime import datetime, date\\nimport pandas as pd\\nfrom pyspark.sql import Row, Column\\nfrom pyspark.sql.functions import upper, pandas_udf\\n\\ndf = spark.createDataFrame([\\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\\n])\\n\\ndf1 = spark.createDataFrame([(2, 3., 'stringtest1', date(2001,3,2), datetime(2001,2,3,13,1))], schema='a long, b double, c string, d date, e timestamp')\\ndf\\n\\ndf1\\n\\ndf.show()\\ndf1.show()\\n\\ndf.printSchema()\\ndf1.printSchema()\\n\\ndf.describe().show()\\n\\ndf.collect()\\n\\ndf.toPandas()\\n\\ndf.a\\n\\ndf.withColumn('UPPER_C', upper(df.c)).show()\\n\\n@pandas_udf('long')\\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\\n    return series+1\\ndf.select(pandas_plus_one(df.a)).show()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row, Column\n",
    "from pyspark.sql.functions import upper, pandas_udf\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "df1 = spark.createDataFrame([(2, 3., 'stringtest1', date(2001,3,2), datetime(2001,2,3,13,1))], schema='a long, b double, c string, d date, e timestamp')\n",
    "df\n",
    "\n",
    "df1\n",
    "\n",
    "df.show()\n",
    "df1.show()\n",
    "\n",
    "df.printSchema()\n",
    "df1.printSchema()\n",
    "\n",
    "df.describe().show()\n",
    "\n",
    "df.collect()\n",
    "\n",
    "df.toPandas()\n",
    "\n",
    "df.a\n",
    "\n",
    "df.withColumn('UPPER_C', upper(df.c)).show()\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    return series+1\n",
    "df.select(pandas_plus_one(df.a)).show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf90e30-1a17-4d72-8202-5ff373143c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning PySpark using E-Commerce Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d841fce4-80ac-47fc-b1ed-f700e03d2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d68369a-6d14-4d5f-bdd3-6843b8fb07b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/06 12:46:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/06 12:46:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"DataCamp PySpark Tutorial\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"10g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69d4f94-39b2-4797-ba49-2d29fcac8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data.csv', header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91671b7-b09b-47bf-acf0-24734ce228eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a93dbf48-4c48-4d8e-b7fa-705fae7870a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309ea8f0-64a1-41f4-88aa-cd3938d98e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer=df.groupby('CustomerID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9f8604-ef8a-4717-a264-f5e6afc3b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|CustomerID|count|\n",
      "+----------+-----+\n",
      "|     16250|   24|\n",
      "|     15574|  168|\n",
      "|     15555|  925|\n",
      "|     15271|  275|\n",
      "|     17714|   10|\n",
      "|     17757|  742|\n",
      "|     17551|   43|\n",
      "|     13187|   37|\n",
      "|     16549|  981|\n",
      "|     12637|  394|\n",
      "|     15052|   30|\n",
      "|     14525|  298|\n",
      "|     18283|  756|\n",
      "|     13107|   60|\n",
      "|     16303|  167|\n",
      "|     13174|  314|\n",
      "|     13027|   26|\n",
      "|     12957|  244|\n",
      "|     17128|   14|\n",
      "|     14439|   32|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf406e3-8109-4f1a-b510-c04ac46985a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4373"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('CustomerID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8c4288-cebf-45ed-8e5f-91e26bc12d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|           Country|country_count|\n",
      "+------------------+-------------+\n",
      "|            Sweden|            8|\n",
      "|         Singapore|            1|\n",
      "|           Germany|           95|\n",
      "|               RSA|            1|\n",
      "|            France|           87|\n",
      "|            Greece|            4|\n",
      "|European Community|            1|\n",
      "|           Belgium|           25|\n",
      "|           Finland|           12|\n",
      "|             Malta|            2|\n",
      "|       Unspecified|            4|\n",
      "|             Italy|           15|\n",
      "|              EIRE|            3|\n",
      "|         Lithuania|            1|\n",
      "|            Norway|           10|\n",
      "|             Spain|           31|\n",
      "|           Denmark|            9|\n",
      "|         Hong Kong|            0|\n",
      "|            Israel|            4|\n",
      "|           Iceland|            1|\n",
      "+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby('Country').agg(count_distinct('CustomerID').alias('country_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7e08cb-68b0-4894-9370-d48b435c95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|        Country| count|\n",
      "+---------------+------+\n",
      "| United Kingdom|495478|\n",
      "|        Germany|  9495|\n",
      "|         France|  8557|\n",
      "|           EIRE|  8196|\n",
      "|          Spain|  2533|\n",
      "|    Netherlands|  2371|\n",
      "|        Belgium|  2069|\n",
      "|    Switzerland|  2002|\n",
      "|       Portugal|  1519|\n",
      "|      Australia|  1259|\n",
      "|         Norway|  1086|\n",
      "|          Italy|   803|\n",
      "|Channel Islands|   758|\n",
      "|        Finland|   695|\n",
      "|         Cyprus|   622|\n",
      "|         Sweden|   462|\n",
      "|    Unspecified|   446|\n",
      "|        Austria|   401|\n",
      "|        Denmark|   389|\n",
      "|          Japan|   358|\n",
      "+---------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Country').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec40cade-a326-4a16-9755-187c023eb7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          max(date)|\n",
      "+-------------------+\n",
      "|2011-12-09 12:50:00|\n",
      "+-------------------+\n",
      "\n",
      "+-------------------+\n",
      "|          min(date)|\n",
      "+-------------------+\n",
      "|2010-12-01 08:26:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=Legacy\")\n",
    "df = df.withColumn('date', to_timestamp(\"InvoiceDate\", 'MM/dd/yy HH:mm'))\n",
    "df.select(max('date')).show()\n",
    "df.select(min('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82309663-ebaa-4198-a569-96b4f05ce0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+-------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|               date|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+-------------------+\n",
      "|   581587|    22613|PACK OF 20 SPACEB...|      12|12/9/2011 12:50|     0.85|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22631|CIRCUS PARADE LUN...|      12|12/9/2011 12:50|     1.95|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22556|PLASTERS IN TIN C...|      12|12/9/2011 12:50|     1.65|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22555|PLASTERS IN TIN S...|      12|12/9/2011 12:50|     1.65|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22728|ALARM CLOCK BAKEL...|       4|12/9/2011 12:50|     3.75|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22727|ALARM CLOCK BAKEL...|       4|12/9/2011 12:50|     3.75|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22726|ALARM CLOCK BAKEL...|       4|12/9/2011 12:50|     3.75|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22730|ALARM CLOCK BAKEL...|       4|12/9/2011 12:50|     3.75|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22367|CHILDRENS APRON S...|       8|12/9/2011 12:50|     1.95|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22629| SPACEBOY LUNCH BOX |      12|12/9/2011 12:50|     1.95|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    23256|CHILDRENS CUTLERY...|       4|12/9/2011 12:50|     4.15|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22899|CHILDREN'S APRON ...|       6|12/9/2011 12:50|      2.1|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    23254|CHILDRENS CUTLERY...|       4|12/9/2011 12:50|     4.15|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    23255|CHILDRENS CUTLERY...|       4|12/9/2011 12:50|     4.15|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581587|    22138|BAKING SET 9 PIEC...|       3|12/9/2011 12:50|     4.95|     12680|        France|2011-12-09 12:50:00|\n",
      "|   581586|    22061|LARGE CAKE STAND ...|       8|12/9/2011 12:49|     2.95|     13113|United Kingdom|2011-12-09 12:49:00|\n",
      "|   581586|    23275|SET OF 3 HANGING ...|      24|12/9/2011 12:49|     1.25|     13113|United Kingdom|2011-12-09 12:49:00|\n",
      "|   581586|    21217|RED RETROSPOT ROU...|      24|12/9/2011 12:49|     8.95|     13113|United Kingdom|2011-12-09 12:49:00|\n",
      "|   581586|    20685|DOORMAT RED RETRO...|      10|12/9/2011 12:49|     7.08|     13113|United Kingdom|2011-12-09 12:49:00|\n",
      "|   581585|    22726|ALARM CLOCK BAKEL...|       8|12/9/2011 12:31|     3.75|     15804|United Kingdom|2011-12-09 12:31:00|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9be5cc1d-bf5d-4f0d-8e9e-83311173b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+-------------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |date               |\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+-------------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|2010-12-01 08:26:00|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01 08:26:00|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|2010-12-01 08:26:00|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01 08:26:00|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01 08:26:00|\n",
      "+---------+---------+-----------------------------------+--------+--------------+---------+----------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e6cfbe-2241-44e1-9f6c-2282a68c86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"from_date\", lit(\"12/1/24 08:26\"))\n",
    "df = df.withColumn('from_date',to_timestamp(\"from_date\", 'yy/MM/dd HH:mm'))\n",
    "\n",
    "df2=df.withColumn('from_date',to_timestamp(col('from_date'))).withColumn('recency',col(\"date\").cast(\"long\") - col('from_date').cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92ab84bb-1df8-4fba-891b-eea167bbdfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|               date|          from_date|  recency|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+---------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|2010-12-01 08:26:00|2012-01-24 08:26:00|-36201600|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01 08:28:00|2012-01-24 08:26:00|-36201480|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|2010-12-01 08:28:00|2012-01-24 08:26:00|-36201480|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|2010-12-01 08:34:00|2012-01-24 08:26:00|-36201120|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49df142b-f973-4e0d-809c-c757812ddc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(df2.groupBy('CustomerID').agg(max('recency').alias('recency')),on='recency',how='leftsemi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbb51ed2-cad9-4ebe-9cb5-cf421835c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+\n",
      "|  recency|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|               date|          from_date|\n",
      "+---------+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+\n",
      "|-36196380|   536384|    82484|WOOD BLACK BOARD ...|       3|12/1/2010 9:53|     6.45|     18074|United Kingdom|2010-12-01 09:53:00|2012-01-24 08:26:00|\n",
      "|-36196380|   536384|    84755|COLOUR GLASS T-LI...|      48|12/1/2010 9:53|     0.65|     18074|United Kingdom|2010-12-01 09:53:00|2012-01-24 08:26:00|\n",
      "|-36196380|   536384|    22464|HANGING METAL HEA...|      12|12/1/2010 9:53|     1.65|     18074|United Kingdom|2010-12-01 09:53:00|2012-01-24 08:26:00|\n",
      "|-36196380|   536384|    21324|HANGING MEDINA LA...|       6|12/1/2010 9:53|     2.95|     18074|United Kingdom|2010-12-01 09:53:00|2012-01-24 08:26:00|\n",
      "|-36196380|   536384|    22457|NATURAL SLATE HEA...|      12|12/1/2010 9:53|     2.95|     18074|United Kingdom|2010-12-01 09:53:00|2012-01-24 08:26:00|\n",
      "+---------+---------+---------+--------------------+--------+--------------+---------+----------+--------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08f394b7-049c-4db0-bcbb-532c6bd91a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- recency: long (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- from_date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ec326c9-9df5-4da6-a0f1-604ae27acdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq = df2.groupBy('CustomerID').agg(count('InvoiceDate').alias('frequency'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14809b57-942a-4bd8-93f5-3c9a22cd9f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 12:47:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 39:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|CustomerID|frequency|\n",
      "+----------+---------+\n",
      "|     17714|       10|\n",
      "|     17551|       43|\n",
      "|     13187|       37|\n",
      "|     15052|       30|\n",
      "|     17128|       14|\n",
      "|     14439|       32|\n",
      "|     18106|       39|\n",
      "|     17855|       17|\n",
      "|     12386|        2|\n",
      "|     13065|       14|\n",
      "|     16510|       13|\n",
      "|     17303|       53|\n",
      "|     15100|        1|\n",
      "|     13328|       17|\n",
      "|     15899|        4|\n",
      "|     16519|       13|\n",
      "|     15350|        5|\n",
      "|     18113|        1|\n",
      "|     13144|        3|\n",
      "|     15180|        6|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e8031d3-fee3-4154-9629-bdde771a6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.join(df_freq,on='CustomerID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cc777f4-333c-4f9a-8e04-b3eac9debacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- recency: long (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- from_date: timestamp (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36a71864-07e9-4677-afab-a0989938229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_val = df3.withColumn('TotalAmount', col('Quantity') * col('UnitPrice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d1c821c-ca77-45a8-b8fb-18c5933730eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_val = m_val.groupby('CustomerID').agg(sum('TotalAmount').alias('monetary_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbfd3bd4-c012-4748-847f-28b25d8f31b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|    monetary_value|\n",
      "+----------+------------------+\n",
      "|     17714|             153.0|\n",
      "|     17551|            306.84|\n",
      "|     13187|236.01999999999995|\n",
      "|     15052|            215.78|\n",
      "|     17128|            157.09|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "m_val.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "128725c5-56eb-4fcf-bc2c-dc7e7203974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = m_val.join(df3,on='CustomerID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e3489f4-7a81-419c-850d-54d11d780da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+---------+---------+---------+--------------------+--------+----------------+---------+--------------+-------------------+-------------------+---------+\n",
      "|CustomerID|monetary_value|  recency|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|       Country|               date|          from_date|frequency|\n",
      "+----------+--------------+---------+---------+---------+--------------------+--------+----------------+---------+--------------+-------------------+-------------------+---------+\n",
      "|     17714|         153.0|-31614840|   541837|    21494|ROTATING LEAVES T...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|    84347|ROTATING SILVER A...|       6| 1/23/2011 10:32|     2.55|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|   85036B|CHOCOLATE 1 WICK ...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|   85034B|3 WHITE CHOC MORR...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|   85035B|CHOCOLATE 3 WICK ...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|   85159A|BLACK TEA,COFFEE,...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|   85180A|RED HEARTS LIGHT ...|      12| 1/23/2011 10:32|     1.25|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|    22317|FIVE CATS HANGING...|       6| 1/23/2011 10:32|     2.95|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|    84580|MOUSE TOY WITH PI...|       4| 1/23/2011 10:32|     3.75|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17714|         153.0|-31614840|   541837|    84581|DOG TOY WITH PINK...|       4| 1/23/2011 10:32|     3.75|United Kingdom|2011-01-23 10:32:00|2012-01-24 08:26:00|       10|\n",
      "|     17551|        306.84|-34971240|   539019|   46000P|POLYESTER FILLER ...|       4|12/15/2010 14:12|     4.25|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|   82494L|WOODEN FRAME ANTI...|       1|12/15/2010 14:12|     2.95|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    21232|STRAWBERRY CERAMI...|       1|12/15/2010 14:12|     1.25|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    22693|GROW A FLYTRAP OR...|      24|12/15/2010 14:12|     1.25|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|   84536A|ENGLISH ROSE NOTE...|       7|12/15/2010 14:12|     0.42|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|   85123A|WHITE HANGING HEA...|       4|12/15/2010 14:12|     2.95|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    84836|ZINC METAL HEART ...|       6|12/15/2010 14:12|     1.25|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    22150|3 STRIPEY MICE FE...|       2|12/15/2010 14:12|     1.95|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    22149|FELTCRAFT 6 FLOWE...|       2|12/15/2010 14:12|      2.1|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "|     17551|        306.84|-34971240|   539019|    20980|36 PENCILS TUBE POSY|       3|12/15/2010 14:12|     1.25|United Kingdom|2010-12-15 14:12:00|2012-01-24 08:26:00|       43|\n",
      "+----------+--------------+---------+---------+---------+--------------------+--------+----------------+---------+--------------+-------------------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aa5587b-d328-420f-8f0f-a8e4c22fc544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91389"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45260bd8-8516-4802-8f7d-70f057e8df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12c41ca8-6b6c-4d0a-9524-4bb057ce94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assemble=VectorAssembler(inputCols=[\n",
    "    'recency','frequency','monetary_value'\n",
    "], outputCol='features')\n",
    "\n",
    "assembled_data=assemble.transform(final_df)\n",
    "\n",
    "scale=StandardScaler(inputCol='features',outputCol='standardized')\n",
    "data_scale=scale.fit(assembled_data)\n",
    "data_scale_output=data_scale.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "324681ab-7691-4f87-88e3-2815dcd1acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+\n",
      "|standardized                                                 |\n",
      "+-------------------------------------------------------------+\n",
      "|[-3.8424347139456296,0.14708124724770674,0.12614330373966906]|\n",
      "|[-3.8424347139456296,0.14708124724770674,0.12614330373966906]|\n",
      "+-------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_scale_output.select('standardized').show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "891aab63-47ee-4e62-8a24-cfcc4f9f5ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"select count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6bc67f8-b52a-4e32-8ec7-878a8de38e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/06 12:49:55 ERROR ArrowPythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Users/veera/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1225, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/Users/veera/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/06 12:49:55 ERROR ArrowPythonRunner: This may have been caused by a prior exception:\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/06 12:49:55 ERROR Executor: Exception in task 0.0 in stage 116.0 (TID 314)\n",
      "java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "24/07/06 12:49:55 WARN TaskSetManager: Lost task 0.0 in stage 116.0 (TID 314) (veeras-mbp executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
      "\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n",
      "\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n",
      "\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "24/07/06 12:49:55 ERROR TaskSetManager: Task 0 in stage 116.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o242.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 314) (veeras-mbp executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/87/hjx2bxk1459c99nqq165988r0000gn/T/ipykernel_22688/2805082804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'add_one(CustomerID)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 116.0 failed 1 times, most recent failure: Lost task 0.0 in stage 116.0 (TID 314) (veeras-mbp executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n\tat org.apache.arrow.memory.util.MemoryUtil.directBuffer(MemoryUtil.java:174)\n\tat org.apache.arrow.memory.ArrowBuf.getDirectBuffer(ArrowBuf.java:229)\n\tat org.apache.arrow.memory.ArrowBuf.nioBuffer(ArrowBuf.java:224)\n\tat org.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:133)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeRecordBatch(ArrowWriter.java:147)\n\tat org.apache.arrow.vector.ipc.ArrowWriter.writeBatch(ArrowWriter.java:133)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:140)\n\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "@pandas_udf(\"integer\")\n",
    "def add_one(s:pd.Series) -> pd.Series:\n",
    "    return s+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0fd07-96f7-4796-b302-6207944f530d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
